from langchain.output_parsers import PydanticToolsParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.pydantic_v1 import BaseModel, Field
from config import load_config
from typing import Literal, List, Dict

config = load_config()
# Define the SubQuery model for individual sub-questions
class SubQuery(BaseModel):
    """Represents a very specific query against the database."""
    sub_query: str = Field(
        ...,
        description="A very specific sub question.",
    )


# System prompt with query decomposition instructions
system_prompt = f"""You are an expert at converting user questions into smaller sub questions. \

Perform query decomposition. Given a user question, break it down into distinct sub questions that \
you need to answer in order to answer the original question.

If there are acronyms or words you are not familiar with, do not try to rephrase them."""

# Set up the ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{question}"),
    ]
)

# Initialize the language model and bind it with the SubQuery tool
llm = ChatOpenAI(openai_api_key=config["OPENAI_API_KEY"], model="gpt-3.5-turbo-0125", temperature=0)
llm_with_tools = llm.bind_tools([SubQuery])

# Set up the parser to handle SubQuery output
parser = PydanticToolsParser(tools=[SubQuery])

# Combine prompt, LLM with tools, and parser into a query analyzer
query_analyzer = prompt | llm_with_tools | parser

# Define the function to analyze the query
def analyze_query(question: str) -> List[str]:
    """
    Analyzes the given question by breaking it down into sub-queries.

    :param question: The main question to be analyzed.
    :return: A list of sub-queries generated to answer the main question.
    """
    # Invoke the query analyzer with the input question
    output = query_analyzer.invoke({"question": question})

    # Extract and return the sub-queries
    return [subquery.sub_query for subquery in output]

# # Define RouteQuery model
# class RouteQuery(BaseModel):
#     """Route a user query to the most relevant datasource."""
#     datasource: Literal["metadata_fields", "transcript_fields"] = Field(
#         ...,
#         description="Given a question choose which datasource would be most relevant for answering the question",
#     )

# # Initialize ChatOpenAI with structured output for routing queries
# llm = ChatOpenAI(openai_api_key=config["OPENAI_API_KEY"], model="gpt-3.5-turbo-0125", temperature=0)
# structured_llm = llm.with_structured_output(RouteQuery)

# # System prompt template for data source routing
# system = """You are an expert at routing a user question to the appropriate data source.

# Based on the programming language the question is referring to, route it to the relevant data source.

# metadata_fields = 
# - Call Length: The duration of the call in seconds.
# - Lead Id: Unique identifier for the lead.
# - Call DateTime: The date and time when the call took place.
# - Language of the call: The language in which the call was conducted. Valid values are ['Hindi', 'Marathi'].
# - Purpose: The purpose of the call, such as 'Construction', 'Other Loans - Home Equity', 'Purchase', etc.
# - Product offered: The product offered during the call.
# - Lead Source: The source from which the lead was generated, such as 'PhonePe', 'Google Ads', 'Self Sourced', etc.
# - Location: The location of the customer.
# - Branch: The branch associated with the lead.
# - Opportunity Created: Indicates if an opportunity was created from the call.
# - Business Created: Indicates if a business was created from the call.
# - Agent Name: The name of the agent handling the call.
# - Agent ID: Unique identifier for the agent.

# transcript_fields = Call transcripts Recordings where it records who has said what
# """
# prompt = ChatPromptTemplate.from_messages(
#     [
#         ("system", system),
#         ("human", "{question}"),
#     ]
# )

# # Combine prompt with structured LLM to create router
# router = prompt | structured_llm

# # Define function to route queries based on sub-queries
# def route_queries(sub_queries: List[str]) -> Dict[str, List[str]]:
#     """
#     Routes each sub-query to the relevant data source (metadata_fields or transcript_fields).
    
#     :param sub_queries: List of sub-queries generated by analyze_query.
#     :return: Dictionary with data sources as keys and lists of sub-queries as values.
#     """
#     # Initialize dictionary to store routed sub-queries
#     routed_queries = {"metadata_fields": [], "transcript_fields": []}
    
#     # Route each sub-query to the relevant data source
#     for query in sub_queries:
#         result = router.invoke({"question": query})
#         data_source = result.datasource  # Either "metadata_fields" or "transcript_fields"
        
#         # Append query to the appropriate data source list
#         routed_queries[data_source].append(query)
    
#     return routed_queries


